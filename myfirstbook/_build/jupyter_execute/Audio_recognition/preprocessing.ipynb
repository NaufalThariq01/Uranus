{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa953e92",
   "metadata": {},
   "source": [
    "# ðŸ§® 5. Data Preprocessing\n",
    "## 5.1 Tujuan\n",
    "\n",
    "Tahap Data Preprocessing bertujuan untuk mengubah data audio mentah menjadi data numerik yang siap digunakan untuk proses modeling. Proses ini meliputi pembacaan file audio, normalisasi sinyal, penghapusan noise, pemotongan bagian diam, serta ekstraksi fitur statistik time series seperti mean, standard deviation, RMS, dan zero crossing rate (ZCR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe10064",
   "metadata": {},
   "source": [
    "## 5.2 Langkah-Langkah Preprocessing\n",
    "### a. Import Library yang Dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1daa353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf       \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60d332",
   "metadata": {},
   "source": [
    "## b.Lakukan preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78bd659",
   "metadata": {},
   "source": [
    "membuat fungsi preprocessing audio, yang akan:\n",
    "\n",
    "âœ… Membaca semua file .wav dari masing-masing orang dan kelas\n",
    "\n",
    "âœ… Normalisasi amplitudo\n",
    "\n",
    "âœ… Trimming bagian diam\n",
    "\n",
    "âœ… Zero-padding agar durasi seragam (misalnya 2 detik @16kHz = 32000 sampel)\n",
    "\n",
    "âœ… Simpan hasil bersih ke folder preprocces_person1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc25a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¤ Memproses: Dataset_Voice_pertama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person1/Buka:   0%|                              | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person1/Buka:   2%|â–                     | 1/50 [00:01<01:37,  2.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person1/Buka:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 30/50 [00:02<00:01, 19.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person1/Buka: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:02<00:00, 22.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person1/Tutup:   0%|                             | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person1/Tutup:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 23/50 [00:00<00:00, 224.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person1/Tutup:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/50 [00:00<00:00, 242.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person1/Tutup: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 234.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ™ï¸ Memproses: Dataset_Voice_kedua\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person2/Buka:   0%|                              | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person2/Buka:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 21/49 [00:00<00:00, 204.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person2/Buka:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 42/49 [00:00<00:00, 197.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person2/Buka: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:00<00:00, 197.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person2/Tutup:   0%|                             | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person2/Tutup:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 26/48 [00:00<00:00, 249.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Person2/Tutup: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 250.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Preprocessing selesai.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================\n",
    "# PATH DATASET\n",
    "# ==========================\n",
    "DATASET_PERSON1 = r\"D:\\KULIAH\\SEMESTER 5\\Program Saint Data\\Uranus\\myfirstbook\\Audio_recognition\\Dataset_Voice_pertama\"\n",
    "DATASET_PERSON2 = r\"D:\\KULIAH\\SEMESTER 5\\Program Saint Data\\Uranus\\myfirstbook\\Audio_recognition\\Dataset_Voice_kedua\"\n",
    "\n",
    "# ==========================\n",
    "# PARAMETER PREPROCESSING\n",
    "# ==========================\n",
    "TARGET_SR = 16000          \n",
    "TARGET_DURATION = 3.0      \n",
    "TARGET_LEN = int(TARGET_SR * TARGET_DURATION)\n",
    "\n",
    "# ==========================\n",
    "# FUNGSI PREPROCESSING\n",
    "# ==========================\n",
    "def preprocess_audio(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=TARGET_SR)\n",
    "        y = librosa.util.normalize(y)\n",
    "        y, _ = librosa.effects.trim(y, top_db=25)\n",
    "\n",
    "        # Balancing durasi\n",
    "        if len(y) < TARGET_LEN:\n",
    "            y = np.pad(y, (0, TARGET_LEN - len(y)), mode='constant')\n",
    "        else:\n",
    "            y = y[:TARGET_LEN]\n",
    "\n",
    "        return y\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error memproses {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==========================\n",
    "# PREPROCESS DATASET PERSON 1\n",
    "# ==========================\n",
    "print(\"\\nðŸŽ¤ Memproses: Dataset_Voice_pertama\")\n",
    "data_person1 = {}  # dictionary: {'buka': [array, ...], 'tutup': [array, ...]}\n",
    "\n",
    "valid_folders = [f for f in os.listdir(DATASET_PERSON1) if f.endswith(\"_wav\")]\n",
    "\n",
    "for label_folder in valid_folders:\n",
    "    label_path = os.path.join(DATASET_PERSON1, label_folder)\n",
    "    if not os.path.isdir(label_path):\n",
    "        continue\n",
    "\n",
    "    label_clean = label_folder.replace(\"_wav\", \"\")\n",
    "    data_person1[label_clean] = []\n",
    "\n",
    "    for file in tqdm(os.listdir(label_path), desc=f\"Person1/{label_clean}\"):\n",
    "        if file.endswith(\".wav\"):\n",
    "            in_path = os.path.join(label_path, file)\n",
    "            y = preprocess_audio(in_path)\n",
    "            if y is not None:\n",
    "                data_person1[label_clean].append(y)\n",
    "\n",
    "# ==========================\n",
    "# PREPROCESS DATASET PERSON 2\n",
    "# ==========================\n",
    "print(\"\\nðŸŽ™ï¸ Memproses: Dataset_Voice_kedua\")\n",
    "data_person2 = {}  # dictionary: {'buka': [array, ...], 'tutup': [array, ...]}\n",
    "\n",
    "for label_folder in os.listdir(DATASET_PERSON2):\n",
    "    label_path = os.path.join(DATASET_PERSON2, label_folder)\n",
    "    if not os.path.isdir(label_path):\n",
    "        continue\n",
    "\n",
    "    data_person2[label_folder] = []\n",
    "\n",
    "    for file in tqdm(os.listdir(label_path), desc=f\"Person2/{label_folder}\"):\n",
    "        if file.endswith(\".wav\"):\n",
    "            in_path = os.path.join(label_path, file)\n",
    "            y = preprocess_audio(in_path)\n",
    "            if y is not None:\n",
    "                data_person2[label_folder].append(y)\n",
    "\n",
    "print(\"\\nâœ… Preprocessing selesai.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457460a6",
   "metadata": {},
   "source": [
    "### c. Fungsi Ekstraksi Fitur Statistik\n",
    "\n",
    "Fungsi ini akan membaca setiap file .wav, melakukan preprocessing dasar, lalu menghitung fitur statistik dari sinyal time domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e776551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "def extract_features(y, sr=SAMPLE_RATE):\n",
    "    features = []\n",
    "\n",
    "    # 1. Statistik dasar\n",
    "    features += [\n",
    "        np.mean(y),\n",
    "        np.std(y),\n",
    "        np.var(y),\n",
    "        np.mean((y - np.mean(y))**3)/(np.std(y)**3 + 1e-6),\n",
    "        np.mean((y - np.mean(y))**4)/(np.std(y)**4 + 1e-6),\n",
    "        np.sqrt(np.mean(y**2)),\n",
    "        np.mean(librosa.feature.zero_crossing_rate(y)),\n",
    "        np.std(librosa.feature.zero_crossing_rate(y)),\n",
    "        np.max(y) - np.min(y)\n",
    "    ]\n",
    "\n",
    "    # 2. Spektral\n",
    "    spec_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    spec_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    spec_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    spec_flatness = librosa.feature.spectral_flatness(y=y)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "\n",
    "    features += [\n",
    "        np.mean(spec_centroid), np.std(spec_centroid),\n",
    "        np.mean(spec_bandwidth), np.std(spec_bandwidth),\n",
    "        np.mean(spec_contrast), np.std(spec_contrast),\n",
    "        np.mean(spec_rolloff), np.std(spec_rolloff),\n",
    "        np.mean(spec_flatness), np.std(spec_flatness),\n",
    "        np.mean(chroma), np.std(chroma)\n",
    "    ]\n",
    "\n",
    "    # 3. MFCC (40 koef)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "\n",
    "    for i in range(40):\n",
    "        features.append(np.mean(mfcc[i]))\n",
    "    for i in range(40):\n",
    "        features.append(np.std(mfcc[i]))\n",
    "\n",
    "    # 4. Delta MFCC (40)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "\n",
    "    for i in range(40):\n",
    "        features.append(np.mean(mfcc_delta[i]))\n",
    "    for i in range(40):\n",
    "        features.append(np.std(mfcc_delta[i]))\n",
    "\n",
    "    # 5. Temporal / Energy\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    features.append(np.mean(rms))\n",
    "    features.append(np.std(rms))\n",
    "\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    features.append(np.mean(onset_env))\n",
    "    features.append(np.std(onset_env))\n",
    "\n",
    "    tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n",
    "    features.append(float(tempo[0]))\n",
    "\n",
    "    autocorr = np.correlate(y, y, mode='full')\n",
    "    mid = len(autocorr)//2\n",
    "    features.append(np.argmax(autocorr[mid+1:]) + 1)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121b387",
   "metadata": {},
   "source": [
    "### d. Looping untuk Mengambil Fitur dari Semua File\n",
    "\n",
    "Kita akan mengambil semua file dari folder buka dan tutup, lalu menambahkan label untuk tiap file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "052c4643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18896\\4033610634.py:66: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, audio_list \u001b[38;5;129;01min\u001b[39;00m data_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m y_audio \u001b[38;5;129;01min\u001b[39;00m audio_list:\n\u001b[1;32m---> 14\u001b[0m         feats \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_audio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m         data_rows\u001b[38;5;241m.\u001b[39mappend(feats)\n\u001b[0;32m     16\u001b[0m         labels\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperson_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n",
      "Cell \u001b[1;32mIn[3], line 69\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(y, sr)\u001b[0m\n\u001b[0;32m     66\u001b[0m tempo \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mbeat\u001b[38;5;241m.\u001b[39mtempo(onset_envelope\u001b[38;5;241m=\u001b[39monset_env, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[0;32m     67\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(tempo[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m---> 69\u001b[0m autocorr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrelate\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(autocorr)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     71\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39margmax(autocorr[mid\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\numeric.py:731\u001b[0m, in \u001b[0;36mcorrelate\u001b[1;34m(a, v, mode)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_correlate_dispatcher)\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcorrelate\u001b[39m(a, v, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    662\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;124;03m    Cross-correlation of two 1-dimensional sequences.\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    729\u001b[0m \n\u001b[0;32m    730\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultiarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrelate2\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_rows = []\n",
    "labels = []\n",
    "\n",
    "all_data = [\n",
    "    (\"naufal\", data_person1),\n",
    "    (\"harits\", data_person2)\n",
    "]\n",
    "\n",
    "for person_tag, data_dict in all_data:\n",
    "    for label, audio_list in data_dict.items():\n",
    "        for y_audio in audio_list:\n",
    "            feats = extract_features(y_audio)\n",
    "            data_rows.append(feats)\n",
    "            labels.append(f\"{person_tag}_{label}\") \n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 1. Kolom Statistik dasar\n",
    "# ==========================\n",
    "columns = [\n",
    "    \"mean\",\"std\",\"var\",\"skew\",\"kurtosis\",\"rms_global\",\n",
    "    \"zcr_mean\",\"zcr_std\",\"amplitude_range\"\n",
    "]\n",
    "\n",
    "# ==========================\n",
    "# 2. Spektral\n",
    "# ==========================\n",
    "columns += [\n",
    "    \"spec_centroid_mean\",\"spec_centroid_std\",\n",
    "    \"spec_bandwidth_mean\",\"spec_bandwidth_std\",\n",
    "    \"spec_contrast_mean\",\"spec_contrast_std\",\n",
    "    \"spec_rolloff_mean\",\"spec_rolloff_std\",\n",
    "    \"spec_flatness_mean\",\"spec_flatness_std\",\n",
    "    \"chroma_mean\",\"chroma_std\"\n",
    "]\n",
    "\n",
    "# ==========================\n",
    "# 3. MFCC 40 (mean + std)\n",
    "# ==========================\n",
    "for i in range(1, 41):\n",
    "    columns.append(f\"mfcc_{i}_mean\")\n",
    "\n",
    "for i in range(1, 41):\n",
    "    columns.append(f\"mfcc_{i}_std\")\n",
    "\n",
    "# ==========================\n",
    "# 4. Delta MFCC 40 (mean + std)\n",
    "# ==========================\n",
    "for i in range(1, 41):\n",
    "    columns.append(f\"mfcc_delta_{i}_mean\")\n",
    "\n",
    "for i in range(1, 41):\n",
    "    columns.append(f\"mfcc_delta_{i}_std\")\n",
    "\n",
    "# ==========================\n",
    "# 5. Temporal / Energy\n",
    "# ==========================\n",
    "columns += [\n",
    "    \"rms_mean\",\"rms_std\",\n",
    "    \"onset_mean\",\"onset_std\",\n",
    "    \"tempo\",\n",
    "    \"autocorr_lag\"\n",
    "]\n",
    "\n",
    "# ==========================\n",
    "# SIMPAN CSV\n",
    "# ==========================\n",
    "df = pd.DataFrame(data_rows, columns=columns)\n",
    "df[\"label\"] = labels\n",
    "\n",
    "OUTPUT_CSV = r\"D:\\KULIAH\\SEMESTER 5\\Program Saint Data\\Uranus\\myfirstbook\\Audio_recognition\\features_audio.csv\"\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Fitur disimpan ke {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27980b94",
   "metadata": {},
   "source": [
    "### e. Analisa dan pemilihan Fitur terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc65cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "def feature_importance_analysis(\n",
    "        X_train, y_train, \n",
    "        X_val, y_val, \n",
    "        cumulative_threshold=0.95\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Analisis feature importance menggunakan Random Forest.\n",
    "    cumulative_threshold: batas cumulative importance (default: 95%)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Training Random Forest untuk analisis feature importance...\")\n",
    "    \n",
    "    rf_temp = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_temp.fit(X_train, y_train)\n",
    "\n",
    "    # â— Ranking feature importance\n",
    "    importances = pd.Series(\n",
    "        rf_temp.feature_importances_, \n",
    "        index=X_train.columns\n",
    "    ).sort_values(ascending=False)\n",
    "\n",
    "    # â— Hitung cumulative importance secara benar\n",
    "    cumulative = importances.cumsum()\n",
    "\n",
    "    # Ambil fitur sampai cumulative >= threshold\n",
    "    selected_features = cumulative[cumulative <= cumulative_threshold].index.tolist()\n",
    "\n",
    "    # Jika pas berhenti di tengah, tambahkan 1 fitur berikutnya agar menutup threshold\n",
    "    if len(selected_features) < len(importances):\n",
    "        next_feat = importances.index[len(selected_features)]\n",
    "        selected_features.append(next_feat)\n",
    "\n",
    "    # Failsafe kalau masih kosong\n",
    "    if len(selected_features) == 0:\n",
    "        selected_features = [importances.idxmax()]\n",
    "\n",
    "    removed_features = [f for f in X_train.columns if f not in selected_features]\n",
    "\n",
    "    # â— Plot cumulative importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(np.arange(len(importances)) + 1, cumulative.values, marker='o')\n",
    "    plt.axhline(y=cumulative_threshold, color='red', linestyle='--')\n",
    "    plt.xlabel(\"Jumlah fitur (urutan importance)\")\n",
    "    plt.ylabel(\"Cumulative importance\")\n",
    "    plt.title(\"Cumulative Feature Importance (Random Forest)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # â— Summary\n",
    "    print(\"\\nHASIL ANALISIS FEATURE IMPORTANCE\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total fitur awal      : {len(importances)}\")\n",
    "    print(f\"Fitur terpilih        : {len(selected_features)}\")\n",
    "    print(f\"Fitur terhapus        : {len(removed_features)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # â— Dataset dengan fitur terpilih\n",
    "    X_train_sel = X_train[selected_features].copy()\n",
    "    X_val_sel   = X_val[selected_features].copy()\n",
    "\n",
    "    return {\n",
    "        \"X_train_selected\": X_train_sel,\n",
    "        \"X_val_selected\": X_val_sel,\n",
    "        \"selected_features\": selected_features,\n",
    "        \"removed_features\": removed_features,\n",
    "        \"feature_importances\": importances\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
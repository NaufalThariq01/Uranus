{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa953e92",
   "metadata": {},
   "source": [
    "# ðŸ§® 5. Data Preprocessing\n",
    "## 5.1 Tujuan\n",
    "\n",
    "Tahap Data Preprocessing bertujuan untuk mengubah data audio mentah menjadi data numerik yang siap digunakan untuk proses modeling. Proses ini meliputi pembacaan file audio, normalisasi sinyal, penghapusan noise, pemotongan bagian diam, serta ekstraksi fitur statistik time series seperti mean, standard deviation, RMS, dan zero crossing rate (ZCR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe10064",
   "metadata": {},
   "source": [
    "## 5.2 Langkah-Langkah Preprocessing\n",
    "### a. Import Library yang Dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1daa353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf       \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60d332",
   "metadata": {},
   "source": [
    "## b.Lakukan preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78bd659",
   "metadata": {},
   "source": [
    "membuat fungsi preprocessing audio, yang akan:\n",
    "\n",
    "âœ… Membaca semua file .wav dari masing-masing orang dan kelas\n",
    "\n",
    "âœ… Normalisasi amplitudo\n",
    "\n",
    "âœ… Trimming bagian diam\n",
    "\n",
    "âœ… Zero-padding agar durasi seragam (misalnya 2 detik @16kHz = 32000 sampel)\n",
    "\n",
    "âœ… Simpan hasil bersih ke folder preprocces_person1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc25a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¤ Memproses: Dataset_Voice_pertama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Person1/Buka: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:08<00:00,  6.23it/s]\n",
      "Person1/Tutup: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:01<00:00, 42.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ™ï¸ Memproses: Dataset_Voice_kedua\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Person2/Buka: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:01<00:00, 44.33it/s]\n",
      "Person2/Tutup: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 52.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Preprocessing selesai.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================\n",
    "# PATH DATASET\n",
    "# ==========================\n",
    "DATASET_PERSON1 = r\"D:\\KULIAH\\SEMESTER 5\\Program Saint Data\\Uranus\\myfirstbook\\Audio_recognition\\Dataset_Voice_pertama\"\n",
    "DATASET_PERSON2 = r\"D:\\KULIAH\\SEMESTER 5\\Program Saint Data\\Uranus\\myfirstbook\\Audio_recognition\\Dataset_Voice_kedua\"\n",
    "\n",
    "# ==========================\n",
    "# PARAMETER PREPROCESSING\n",
    "# ==========================\n",
    "TARGET_SR = 16000          \n",
    "TARGET_DURATION = 3.0      \n",
    "TARGET_LEN = int(TARGET_SR * TARGET_DURATION)\n",
    "\n",
    "# ==========================\n",
    "# FUNGSI PREPROCESSING\n",
    "# ==========================\n",
    "def preprocess_audio(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=TARGET_SR)\n",
    "        y = librosa.util.normalize(y)\n",
    "        y, _ = librosa.effects.trim(y, top_db=25)\n",
    "\n",
    "        # Balancing durasi\n",
    "        if len(y) < TARGET_LEN:\n",
    "            y = np.pad(y, (0, TARGET_LEN - len(y)), mode='constant')\n",
    "        else:\n",
    "            y = y[:TARGET_LEN]\n",
    "\n",
    "        return y\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error memproses {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==========================\n",
    "# PREPROCESS DATASET PERSON 1\n",
    "# ==========================\n",
    "print(\"\\nðŸŽ¤ Memproses: Dataset_Voice_pertama\")\n",
    "data_person1 = {}  # dictionary: {'buka': [array, ...], 'tutup': [array, ...]}\n",
    "\n",
    "valid_folders = [f for f in os.listdir(DATASET_PERSON1) if f.endswith(\"_wav\")]\n",
    "\n",
    "for label_folder in valid_folders:\n",
    "    label_path = os.path.join(DATASET_PERSON1, label_folder)\n",
    "    if not os.path.isdir(label_path):\n",
    "        continue\n",
    "\n",
    "    label_clean = label_folder.replace(\"_wav\", \"\")\n",
    "    data_person1[label_clean] = []\n",
    "\n",
    "    for file in tqdm(os.listdir(label_path), desc=f\"Person1/{label_clean}\"):\n",
    "        if file.endswith(\".wav\"):\n",
    "            in_path = os.path.join(label_path, file)\n",
    "            y = preprocess_audio(in_path)\n",
    "            if y is not None:\n",
    "                data_person1[label_clean].append(y)\n",
    "\n",
    "# ==========================\n",
    "# PREPROCESS DATASET PERSON 2\n",
    "# ==========================\n",
    "print(\"\\nðŸŽ™ï¸ Memproses: Dataset_Voice_kedua\")\n",
    "data_person2 = {}  # dictionary: {'buka': [array, ...], 'tutup': [array, ...]}\n",
    "\n",
    "for label_folder in os.listdir(DATASET_PERSON2):\n",
    "    label_path = os.path.join(DATASET_PERSON2, label_folder)\n",
    "    if not os.path.isdir(label_path):\n",
    "        continue\n",
    "\n",
    "    data_person2[label_folder] = []\n",
    "\n",
    "    for file in tqdm(os.listdir(label_path), desc=f\"Person2/{label_folder}\"):\n",
    "        if file.endswith(\".wav\"):\n",
    "            in_path = os.path.join(label_path, file)\n",
    "            y = preprocess_audio(in_path)\n",
    "            if y is not None:\n",
    "                data_person2[label_folder].append(y)\n",
    "\n",
    "print(\"\\nâœ… Preprocessing selesai.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457460a6",
   "metadata": {},
   "source": [
    "### c. Fungsi Ekstraksi Fitur Statistik\n",
    "\n",
    "Fungsi ini akan membaca setiap file .wav, melakukan preprocessing dasar, lalu menghitung fitur statistik dari sinyal time domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e776551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "\n",
    "def extract_features(y, sr=SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Ekstraksi fitur lengkap + MFCC 40 koef.\n",
    "    Output: list fitur\n",
    "    \"\"\"\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # -------------------\n",
    "    # 1. Statistik dasar\n",
    "    # -------------------\n",
    "    features += [\n",
    "        np.mean(y),\n",
    "        np.std(y),\n",
    "        np.var(y),\n",
    "        np.mean((y - np.mean(y))**3)/(np.std(y)**3 + 1e-6),  # skewness\n",
    "        np.mean((y - np.mean(y))**4)/(np.std(y)**4 + 1e-6),  # kurtosis\n",
    "        np.sqrt(np.mean(y**2)),  # RMS global\n",
    "        np.mean(librosa.feature.zero_crossing_rate(y)),\n",
    "        np.std(librosa.feature.zero_crossing_rate(y)),\n",
    "        np.max(y) - np.min(y)\n",
    "    ]\n",
    "\n",
    "    # -------------------\n",
    "    # 2. Spektral\n",
    "    # -------------------\n",
    "    spec_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    spec_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    spec_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    spec_flatness = librosa.feature.spectral_flatness(y=y)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "\n",
    "    features += [\n",
    "        np.mean(spec_centroid), np.std(spec_centroid),\n",
    "        np.mean(spec_bandwidth), np.std(spec_bandwidth),\n",
    "        np.mean(spec_contrast), np.std(spec_contrast),\n",
    "        np.mean(spec_rolloff), np.std(spec_rolloff),\n",
    "        np.mean(spec_flatness), np.std(spec_flatness),\n",
    "        np.mean(chroma), np.std(chroma)\n",
    "    ]\n",
    "\n",
    "    # -------------------\n",
    "    # 3. MFCC 40 koef\n",
    "    # -------------------\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "\n",
    "    # Mean tiap koef MFCC\n",
    "    for i in range(40):\n",
    "        features.append(np.mean(mfcc[i]))\n",
    "\n",
    "    # Std tiap koef MFCC\n",
    "    for i in range(40):\n",
    "        features.append(np.std(mfcc[i]))\n",
    "\n",
    "    # -------------------\n",
    "    # 4. Delta MFCC 40 koef\n",
    "    # -------------------\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "\n",
    "    for i in range(40):\n",
    "        features.append(np.mean(mfcc_delta[i]))\n",
    "\n",
    "    for i in range(40):\n",
    "        features.append(np.std(mfcc_delta[i]))\n",
    "\n",
    "    # -------------------\n",
    "    # 5. Temporal / Energy\n",
    "    # -------------------\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    features.append(np.mean(rms))\n",
    "    features.append(np.std(rms))\n",
    "\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    features.append(np.mean(onset_env))\n",
    "    features.append(np.std(onset_env))\n",
    "\n",
    "    tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n",
    "    features.append(float(tempo[0]))\n",
    "\n",
    "    autocorr = np.correlate(y, y, mode='full')\n",
    "    mid = len(autocorr)//2\n",
    "    features.append(np.argmax(autocorr[mid+1:]) + 1)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121b387",
   "metadata": {},
   "source": [
    "### d. Looping untuk Mengambil Fitur dari Semua File\n",
    "\n",
    "Kita akan mengambil semua file dari folder buka dan tutup, lalu menambahkan label untuk tiap file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "052c4643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_20284\\3300047557.py:80: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Fitur disimpan ke D:\\KULIAH\\SEMESTER 5\\Program Saint Data\\Uranus\\myfirstbook\\Audio_recognition\\features_audio.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_rows = []\n",
    "labels = []\n",
    "\n",
    "all_data = [\n",
    "    (\"naufal\", data_person1),\n",
    "    (\"harits\", data_person2)\n",
    "]\n",
    "\n",
    "for person_tag, data_dict in all_data:\n",
    "    for label, audio_list in data_dict.items():\n",
    "        for y_audio in audio_list:\n",
    "            feats = extract_features(y_audio)\n",
    "            data_rows.append(feats)\n",
    "            labels.append(f\"{person_tag}_{label}\") \n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 1. Kolom Statistik dasar\n",
    "# ==========================\n",
    "columns = [\n",
    "    \"mean\",\"std\",\"var\",\"skew\",\"kurtosis\",\"rms_global\",\n",
    "    \"zcr_mean\",\"zcr_std\",\"amplitude_range\"\n",
    "]\n",
    "\n",
    "# ==========================\n",
    "# 2. Spektral\n",
    "# ==========================\n",
    "columns += [\n",
    "    \"spec_centroid_mean\",\"spec_centroid_std\",\n",
    "    \"spec_bandwidth_mean\",\"spec_bandwidth_std\",\n",
    "    \"spec_contrast_mean\",\"spec_contrast_std\",\n",
    "    \"spec_rolloff_mean\",\"spec_rolloff_std\",\n",
    "    \"spec_flatness_mean\",\"spec_flatness_std\",\n",
    "    \"chroma_mean\",\"chroma_std\"\n",
    "]\n",
    "\n",
    "# ==========================\n",
    "# 3. MFCC 40 (mean + std)\n",
    "# ==========================\n",
    "for i in range(1, 41):\n",
    "    columns.append(f\"mfcc_{i}_mean\")\n",
    "\n",
    "for i in range(1, 41):\n",
    "    columns.append(f\"mfcc_{i}_std\")\n",
    "\n",
    "# ==========================\n",
    "# 4. Delta MFCC 40 (mean + std)\n",
    "# ==========================\n",
    "for i in range(1, 41):\n",
    "    columns.append(f\"mfcc_delta_{i}_mean\")\n",
    "\n",
    "for i in range(1, 41):\n",
    "    columns.append(f\"mfcc_delta_{i}_std\")\n",
    "\n",
    "# ==========================\n",
    "# 5. Temporal / Energy\n",
    "# ==========================\n",
    "columns += [\n",
    "    \"rms_mean\",\"rms_std\",\n",
    "    \"onset_mean\",\"onset_std\",\n",
    "    \"tempo\",\n",
    "    \"autocorr_lag\"\n",
    "]\n",
    "\n",
    "# ==========================\n",
    "# SIMPAN CSV\n",
    "# ==========================\n",
    "df = pd.DataFrame(data_rows, columns=columns)\n",
    "df[\"label\"] = labels\n",
    "\n",
    "OUTPUT_CSV = r\"D:\\KULIAH\\SEMESTER 5\\Program Saint Data\\Uranus\\myfirstbook\\Audio_recognition\\features_audio.csv\"\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Fitur disimpan ke {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27980b94",
   "metadata": {},
   "source": [
    "### e. Analisa dan pemilihan Fitur terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc65cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "def feature_importance_analysis(\n",
    "        X_train, y_train, \n",
    "        X_val, y_val, \n",
    "        cumulative_threshold=0.95\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Analisis feature importance menggunakan Random Forest.\n",
    "    cumulative_threshold: batas cumulative importance (default: 95%)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Training Random Forest untuk analisis feature importance...\")\n",
    "    \n",
    "    rf_temp = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_temp.fit(X_train, y_train)\n",
    "\n",
    "    # â— Ranking feature importance\n",
    "    importances = pd.Series(\n",
    "        rf_temp.feature_importances_, \n",
    "        index=X_train.columns\n",
    "    ).sort_values(ascending=False)\n",
    "\n",
    "    # â— Hitung cumulative importance secara benar\n",
    "    cumulative = importances.cumsum()\n",
    "\n",
    "    # Ambil fitur sampai cumulative >= threshold\n",
    "    selected_features = cumulative[cumulative <= cumulative_threshold].index.tolist()\n",
    "\n",
    "    # Jika pas berhenti di tengah, tambahkan 1 fitur berikutnya agar menutup threshold\n",
    "    if len(selected_features) < len(importances):\n",
    "        next_feat = importances.index[len(selected_features)]\n",
    "        selected_features.append(next_feat)\n",
    "\n",
    "    # Failsafe kalau masih kosong\n",
    "    if len(selected_features) == 0:\n",
    "        selected_features = [importances.idxmax()]\n",
    "\n",
    "    removed_features = [f for f in X_train.columns if f not in selected_features]\n",
    "\n",
    "    # â— Plot cumulative importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(np.arange(len(importances)) + 1, cumulative.values, marker='o')\n",
    "    plt.axhline(y=cumulative_threshold, color='red', linestyle='--')\n",
    "    plt.xlabel(\"Jumlah fitur (urutan importance)\")\n",
    "    plt.ylabel(\"Cumulative importance\")\n",
    "    plt.title(\"Cumulative Feature Importance (Random Forest)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # â— Summary\n",
    "    print(\"\\nHASIL ANALISIS FEATURE IMPORTANCE\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total fitur awal      : {len(importances)}\")\n",
    "    print(f\"Fitur terpilih        : {len(selected_features)}\")\n",
    "    print(f\"Fitur terhapus        : {len(removed_features)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # â— Dataset dengan fitur terpilih\n",
    "    X_train_sel = X_train[selected_features].copy()\n",
    "    X_val_sel   = X_val[selected_features].copy()\n",
    "\n",
    "    return {\n",
    "        \"X_train_selected\": X_train_sel,\n",
    "        \"X_val_selected\": X_val_sel,\n",
    "        \"selected_features\": selected_features,\n",
    "        \"removed_features\": removed_features,\n",
    "        \"feature_importances\": importances\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

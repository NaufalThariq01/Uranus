{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa953e92",
   "metadata": {},
   "source": [
    "# ðŸ§® 5. Data Preprocessing\n",
    "## 5.1 Tujuan\n",
    "\n",
    "Tahap Data Preprocessing bertujuan untuk mengubah data audio mentah menjadi data numerik yang siap digunakan untuk proses modeling. Proses ini meliputi pembacaan file audio, normalisasi sinyal, penghapusan noise, pemotongan bagian diam, serta ekstraksi fitur statistik time series seperti mean, standard deviation, RMS, dan zero crossing rate (ZCR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe10064",
   "metadata": {},
   "source": [
    "## 5.2 Langkah-Langkah Preprocessing\n",
    "### a. Import Library yang Dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1daa353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf       \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60d332",
   "metadata": {},
   "source": [
    "## b.Lakukan preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78bd659",
   "metadata": {},
   "source": [
    "membuat fungsi preprocessing audio, yang akan:\n",
    "\n",
    "âœ… Membaca semua file .wav dari masing-masing orang dan kelas\n",
    "\n",
    "âœ… Normalisasi amplitudo\n",
    "\n",
    "âœ… Trimming bagian diam\n",
    "\n",
    "âœ… Zero-padding agar durasi seragam (misalnya 2 detik @16kHz = 32000 sampel)\n",
    "\n",
    "âœ… Simpan hasil bersih ke folder preprocces_person1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc25a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¤ Memproses: Dataset_Voice_pertama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Person1/Buka: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:08<00:00,  5.70it/s]\n",
      "Person1/Tutup: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 50.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ™ï¸ Memproses: Dataset_Voice_kedua\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Person2/Buka: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:01<00:00, 44.68it/s]\n",
      "Person2/Tutup: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 50.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Preprocessing selesai.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================\n",
    "# PATH DATASET\n",
    "# ==========================\n",
    "DATASET_PERSON1 = r\"D:\\KULIAH\\SEMESTER 5\\Program Saint Data\\Uranus\\myfirstbook\\Audio_recognition\\Dataset_Voice_pertama\"\n",
    "DATASET_PERSON2 = r\"D:\\KULIAH\\SEMESTER 5\\Program Saint Data\\Uranus\\myfirstbook\\Audio_recognition\\Dataset_Voice_kedua\"\n",
    "\n",
    "# ==========================\n",
    "# PARAMETER PREPROCESSING\n",
    "# ==========================\n",
    "TARGET_SR = 16000          \n",
    "TARGET_DURATION = 3.0      \n",
    "TARGET_LEN = int(TARGET_SR * TARGET_DURATION)\n",
    "\n",
    "# ==========================\n",
    "# FUNGSI PREPROCESSING\n",
    "# ==========================\n",
    "def preprocess_audio(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=TARGET_SR)\n",
    "        y = librosa.util.normalize(y)\n",
    "        y, _ = librosa.effects.trim(y, top_db=25)\n",
    "\n",
    "        # Balancing durasi\n",
    "        if len(y) < TARGET_LEN:\n",
    "            y = np.pad(y, (0, TARGET_LEN - len(y)), mode='constant')\n",
    "        else:\n",
    "            y = y[:TARGET_LEN]\n",
    "\n",
    "        return y\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error memproses {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==========================\n",
    "# PREPROCESS DATASET PERSON 1\n",
    "# ==========================\n",
    "print(\"\\nðŸŽ¤ Memproses: Dataset_Voice_pertama\")\n",
    "data_person1 = {}  # dictionary: {'buka': [array, ...], 'tutup': [array, ...]}\n",
    "\n",
    "valid_folders = [f for f in os.listdir(DATASET_PERSON1) if f.endswith(\"_wav\")]\n",
    "\n",
    "for label_folder in valid_folders:\n",
    "    label_path = os.path.join(DATASET_PERSON1, label_folder)\n",
    "    if not os.path.isdir(label_path):\n",
    "        continue\n",
    "\n",
    "    label_clean = label_folder.replace(\"_wav\", \"\")\n",
    "    data_person1[label_clean] = []\n",
    "\n",
    "    for file in tqdm(os.listdir(label_path), desc=f\"Person1/{label_clean}\"):\n",
    "        if file.endswith(\".wav\"):\n",
    "            in_path = os.path.join(label_path, file)\n",
    "            y = preprocess_audio(in_path)\n",
    "            if y is not None:\n",
    "                data_person1[label_clean].append(y)\n",
    "\n",
    "# ==========================\n",
    "# PREPROCESS DATASET PERSON 2\n",
    "# ==========================\n",
    "print(\"\\nðŸŽ™ï¸ Memproses: Dataset_Voice_kedua\")\n",
    "data_person2 = {}  # dictionary: {'buka': [array, ...], 'tutup': [array, ...]}\n",
    "\n",
    "for label_folder in os.listdir(DATASET_PERSON2):\n",
    "    label_path = os.path.join(DATASET_PERSON2, label_folder)\n",
    "    if not os.path.isdir(label_path):\n",
    "        continue\n",
    "\n",
    "    data_person2[label_folder] = []\n",
    "\n",
    "    for file in tqdm(os.listdir(label_path), desc=f\"Person2/{label_folder}\"):\n",
    "        if file.endswith(\".wav\"):\n",
    "            in_path = os.path.join(label_path, file)\n",
    "            y = preprocess_audio(in_path)\n",
    "            if y is not None:\n",
    "                data_person2[label_folder].append(y)\n",
    "\n",
    "print(\"\\nâœ… Preprocessing selesai.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457460a6",
   "metadata": {},
   "source": [
    "### c. Fungsi Ekstraksi Fitur Statistik\n",
    "\n",
    "Fungsi ini akan membaca setiap file .wav, melakukan preprocessing dasar, lalu menghitung fitur statistik dari sinyal time domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e776551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "def extract_features(y, sr=SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Ekstraksi fitur dari sinyal audio y (numpy array) dengan sample rate sr.\n",
    "    Output: list fitur\n",
    "    \"\"\"\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # -------------------\n",
    "    # 1. Statistik dasar\n",
    "    # -------------------\n",
    "    features += [\n",
    "        np.mean(y),\n",
    "        np.std(y),\n",
    "        np.var(y),\n",
    "        np.mean((y - np.mean(y))**3)/(np.std(y)**3 + 1e-6),  # skewness\n",
    "        np.mean((y - np.mean(y))**4)/(np.std(y)**4 + 1e-6),  # kurtosis\n",
    "        np.sqrt(np.mean(y**2)),  # RMS global\n",
    "        np.mean(librosa.feature.zero_crossing_rate(y)),\n",
    "        np.std(librosa.feature.zero_crossing_rate(y)),\n",
    "        np.max(y) - np.min(y)\n",
    "    ]\n",
    "\n",
    "    # -------------------\n",
    "    # 2. Spektral\n",
    "    # -------------------\n",
    "    spec_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    spec_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    spec_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    spec_flatness = librosa.feature.spectral_flatness(y=y)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "\n",
    "    features += [\n",
    "        np.mean(spec_centroid), np.std(spec_centroid),\n",
    "        np.mean(spec_bandwidth), np.std(spec_bandwidth),\n",
    "        np.mean(spec_contrast), np.std(spec_contrast),\n",
    "        np.mean(spec_rolloff), np.std(spec_rolloff),\n",
    "        np.mean(spec_flatness), np.std(spec_flatness),\n",
    "        np.mean(chroma), np.std(chroma)\n",
    "    ]\n",
    "\n",
    "    # MFCC 13 koef\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    for i in range(13):\n",
    "        features.append(np.mean(mfcc[i]))\n",
    "    for i in range(13):\n",
    "        features.append(np.std(mfcc[i]))\n",
    "\n",
    "    # Delta MFCC\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    for i in range(13):\n",
    "        features.append(np.mean(mfcc_delta[i]))\n",
    "    for i in range(13):\n",
    "        features.append(np.std(mfcc_delta[i]))\n",
    "\n",
    "    # -------------------\n",
    "    # 3. Temporal / Energy\n",
    "    # -------------------\n",
    "    # RMS per frame\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    features.append(np.mean(rms))\n",
    "    features.append(np.std(rms))\n",
    "\n",
    "    # Onset envelope\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    features.append(np.mean(onset_env))\n",
    "    features.append(np.std(onset_env))\n",
    "\n",
    "    # Tempo\n",
    "    tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n",
    "    features.append(float(tempo[0]))\n",
    "\n",
    "    # Autocorrelation (lag of max peak)\n",
    "    autocorr = np.correlate(y, y, mode='full')\n",
    "    mid = len(autocorr)//2\n",
    "    features.append(np.argmax(autocorr[mid+1:]) + 1)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121b387",
   "metadata": {},
   "source": [
    "### d. Looping untuk Mengambil Fitur dari Semua File\n",
    "\n",
    "Kita akan mengambil semua file dari folder buka dan tutup, lalu menambahkan label untuk tiap file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "052c4643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3748\\2807756471.py:73: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Fitur disimpan ke D:\\KULIAH\\SEMESTER 5\\Program Saint Data\\Uranus\\myfirstbook\\Audio_recognition\\features_audio.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_rows = []\n",
    "labels = []\n",
    "\n",
    "# Gabungkan semua data preprocessing di memory\n",
    "all_data = [\n",
    "    (\"person1\", data_person1),\n",
    "    (\"person2\", data_person2)\n",
    "]\n",
    "\n",
    "for person_tag, data_dict in all_data:\n",
    "    for label, audio_list in data_dict.items():\n",
    "        for y_audio in audio_list:\n",
    "            feats = extract_features(y_audio)  # array y langsung\n",
    "            data_rows.append(feats)\n",
    "            labels.append(f\"{person_tag}_{label}\")  # misal: person1_buka\n",
    "\n",
    "# ==========================\n",
    "# Buat nama kolom sesuai fitur baru\n",
    "# ==========================\n",
    "columns = []\n",
    "\n",
    "# Statistik dasar\n",
    "columns += [\"mean\",\"std\",\"var\",\"skew\",\"kurtosis\",\"rms_global\",\"zcr_mean\",\"zcr_std\",\"amplitude_range\"]\n",
    "\n",
    "# Spektral\n",
    "columns += [\n",
    "    \"spec_centroid_mean\",\"spec_centroid_std\",\n",
    "    \"spec_bandwidth_mean\",\"spec_bandwidth_std\",\n",
    "    \"spec_contrast_mean\",\"spec_contrast_std\",\n",
    "    \"spec_rolloff_mean\",\"spec_rolloff_std\",\n",
    "    \"spec_flatness_mean\",\"spec_flatness_std\",\n",
    "    \"chroma_mean\",\"chroma_std\"\n",
    "]\n",
    "\n",
    "# MFCC 13 + delta MFCC 13\n",
    "for i in range(1,14):\n",
    "    columns.append(f\"mfcc{i}_mean\")\n",
    "for i in range(1,14):\n",
    "    columns.append(f\"mfcc{i}_std\")\n",
    "for i in range(1,14):\n",
    "    columns.append(f\"mfcc{i}_delta_mean\")\n",
    "for i in range(1,14):\n",
    "    columns.append(f\"mfcc{i}_delta_std\")\n",
    "\n",
    "# Temporal / Energy\n",
    "columns += [\"rms_mean\",\"rms_std\",\"onset_mean\",\"onset_std\",\"tempo\",\"autocorr_lag\"]\n",
    "\n",
    "# ==========================\n",
    "# Simpan ke CSV\n",
    "# ==========================\n",
    "df = pd.DataFrame(data_rows, columns=columns)\n",
    "df['label'] = labels\n",
    "OUTPUT_CSV = r\"D:\\KULIAH\\SEMESTER 5\\Program Saint Data\\Uranus\\myfirstbook\\Audio_recognition\\features_audio.csv\"\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\nâœ… Fitur disimpan ke {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27980b94",
   "metadata": {},
   "source": [
    "### e. Analisa dan pemilihan Fitur terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc65cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "def feature_importance_analysis(X_train, y_train, X_val, y_val, cumulative_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Analisis feature importance menggunakan Random Forest.\n",
    "    - cumulative_threshold: jumlah cumulative importance untuk memilih fitur terbaik (default 95%)\n",
    "    \"\"\"\n",
    "    print(\"Training Random Forest untuk analisis feature importance...\")\n",
    "    rf_temp = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    rf_temp.fit(X_train, y_train)\n",
    "\n",
    "    # Feature importance\n",
    "    importances = pd.Series(rf_temp.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "    # Cumulative importance\n",
    "    cumulative = importances.cumsum()\n",
    "    selected_features = cumulative[cumulative <= cumulative_threshold].index.tolist()\n",
    "    # Pastikan selalu setidaknya 1 fitur\n",
    "    if len(selected_features) == 0:\n",
    "        selected_features = [importances.idxmax()]\n",
    "\n",
    "    removed_features = [f for f in X_train.columns if f not in selected_features]\n",
    "\n",
    "    # Visualisasi cumulative importance\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(np.arange(len(importances))+1, cumulative.values, marker='o')\n",
    "    plt.axhline(y=cumulative_threshold, color='red', linestyle='--', label=f'Cumulative threshold={cumulative_threshold}')\n",
    "    plt.xlabel('Fitur (urut descending importance)')\n",
    "    plt.ylabel('Cumulative Importance')\n",
    "    plt.title('Cumulative Feature Importance')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nHASIL ANALISIS FEATURE IMPORTANCE\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    print(f\"Total fitur awal: {len(importances)}\")\n",
    "    print(f\"Fitur yang dipilih: {len(selected_features)}\")\n",
    "    print(f\"Fitur yang dihapus: {len(removed_features)}\")\n",
    "\n",
    "    # Dataset terpilih\n",
    "    X_train_selected = X_train[selected_features].copy()\n",
    "    X_val_selected = X_val[selected_features].copy()\n",
    "\n",
    "    return X_train_selected, X_val_selected, selected_features, removed_features, importances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2293285d",
   "metadata": {},
   "source": [
    "## Deteksi Outlier dan penanganannya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98dd8d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ===============================\n",
    "# Fungsi deteksi outlier dengan beberapa metode\n",
    "# ===============================\n",
    "def detect_outliers_multiple_methods(df, contamination=0.05, scale=True):\n",
    "    \"\"\"\n",
    "    Deteksi outlier menggunakan 3 metode:\n",
    "    1. IQR\n",
    "    2. Z-Score\n",
    "    3. Isolation Forest\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=['label'])\n",
    "    \n",
    "    # Optional: standarisasi\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    else:\n",
    "        X_scaled = X.copy()\n",
    "    \n",
    "    # 1. IQR (gunakan data asli)\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_mask_iqr = ((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    \n",
    "    # 2. Z-Score\n",
    "    outlier_mask_zscore = (np.abs(X_scaled) > 3).any(axis=1)\n",
    "    \n",
    "    # 3. Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "    outlier_mask_iso = iso_forest.fit_predict(X_scaled) == -1\n",
    "    \n",
    "    return {\n",
    "        'iqr_outliers': outlier_mask_iqr,\n",
    "        'zscore_outliers': outlier_mask_zscore,\n",
    "        'isolation_outliers': outlier_mask_iso,\n",
    "        'X_scaled': X_scaled\n",
    "    }\n",
    "\n",
    "# ===============================\n",
    "# Analisis dan visualisasi outlier\n",
    "# ===============================\n",
    "def analyze_outliers(df, contamination=0.05, scale=True):\n",
    "    \"\"\"\n",
    "    Analisis outlier per kelas & metode\n",
    "    \"\"\"\n",
    "    print(\"ANALISIS OUTLIER\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    outliers = detect_outliers_multiple_methods(df, contamination=contamination, scale=scale)\n",
    "    \n",
    "    # Hitung jumlah outlier\n",
    "    def print_counts(out_dict):\n",
    "        for method in ['iqr_outliers','zscore_outliers','isolation_outliers']:\n",
    "            count = out_dict[method].sum()\n",
    "            print(f\"{method.replace('_outliers','').capitalize()}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    print_counts(outliers)\n",
    "    \n",
    "    # Visualisasi bar chart per kelas\n",
    "    summary = df.groupby('label').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'IQR': detect_outliers_multiple_methods(x, contamination=contamination, scale=scale)['iqr_outliers'].sum(),\n",
    "            'Z-Score': detect_outliers_multiple_methods(x, contamination=contamination, scale=scale)['zscore_outliers'].sum(),\n",
    "            'Isolation': detect_outliers_multiple_methods(x, contamination=contamination, scale=scale)['isolation_outliers'].sum()\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    melted = pd.melt(summary, id_vars=['label'], var_name='Method', value_name='Count')\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(data=melted, x='label', y='Count', hue='Method')\n",
    "    plt.title('Outliers per Class & Method')\n",
    "    plt.show()\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# ===============================\n",
    "# Hapus outlier dengan consensus minimal 2 metode\n",
    "# ===============================\n",
    "def remove_outliers_consensus(df, outlier_results, min_methods=2):\n",
    "    \"\"\"\n",
    "    Menghapus data yang dianggap outlier oleh minimal min_methods metode\n",
    "    \"\"\"\n",
    "    outlier_counts = (\n",
    "        outlier_results['iqr_outliers'].astype(int) +\n",
    "        outlier_results['zscore_outliers'].astype(int) +\n",
    "        outlier_results['isolation_outliers'].astype(int)\n",
    "    )\n",
    "    consensus_outliers = outlier_counts >= min_methods\n",
    "    clean_df = df[~consensus_outliers].copy()\n",
    "    removed_df = df[consensus_outliers].copy()\n",
    "    \n",
    "    print(f\"\\nOutlier removal dengan consensus >= {min_methods} metode:\")\n",
    "    print(f\"  Original samples: {len(df)}\")\n",
    "    print(f\"  Outliers removed: {len(removed_df)} ({len(removed_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Clean samples: {len(clean_df)} ({len(clean_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nDistribusi kelas:\")\n",
    "    print(f\"  Original: {dict(df['label'].value_counts())}\")\n",
    "    print(f\"  Clean: {dict(clean_df['label'].value_counts())}\")\n",
    "    print(f\"  Removed: {dict(removed_df['label'].value_counts())}\")\n",
    "    \n",
    "    return clean_df, removed_df\n",
    "\n",
    "# ===============================\n",
    "# Contoh eksekusi\n",
    "# ===============================\n",
    "# Misal train_selected dan val_selected sudah ada dari feature selection\n",
    "# train_outliers = analyze_outliers(train_selected, contamination=0.05)\n",
    "# train_clean, train_removed = remove_outliers_consensus(train_selected, train_outliers, min_methods=2)\n",
    "# val_outliers = analyze_outliers(val_selected, contamination=0.05)\n",
    "# val_clean, val_removed = remove_outliers_consensus(val_selected, val_outliers, min_methods=2)\n",
    "\n",
    "# ===============================\n",
    "# Simpan hasil ke CSV\n",
    "# ===============================\n",
    "# train_clean.to_csv(\"train_features_clean.csv\", index=False)\n",
    "# val_clean.to_csv(\"val_features_clean.csv\", index=False)\n",
    "# train_removed.to_csv(\"train_outliers_removed.csv\", index=False)\n",
    "# val_removed.to_csv(\"val_outliers_removed.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
